<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ceph | brunocarvalho.net]]></title>
  <link href="http://brunocarvalho.net/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://brunocarvalho.net/"/>
  <updated>2018-09-11T20:58:03-03:00</updated>
  <id>http://brunocarvalho.net/</id>
  <author>
    <name><![CDATA[Bruno Carvalho]]></name>
    <email><![CDATA[brunowcs@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ceph Scrubbing]]></title>
    <link href="http://brunocarvalho.net/blog/2018/09/11/ceph-scrubbing/"/>
    <updated>2018-09-11T19:03:07-03:00</updated>
    <id>http://brunocarvalho.net/blog/2018/09/11/ceph-scrubbing</id>
    <content type="html"><![CDATA[<p><span style="display:block;text-align:center"><img src="/images/ceph/ceph.png" alt="" /> </span></p>

<p>Recentemente encontrei um bug relacionado a recuperação de PGs dentro do Jewel. O Ceph não estava conseguindo resolver a inconsistência de uma PG, mesmo executando o “ceph repair” e forçando a passagem do Scrub. Pelo que notei nos últimos lançamentos do CEPH tem alguns Bug fix sobre o assunto.</p>

<p>Esse será o primeiro de alguns posts que irei falar de como resolver manualmente uma pg inconsistente. Primeiramente vamos entender o Scrubbing no Ceph e caso tenha alguma duvida sobre o SDS Ceph não deixa de assitir meu Webinar que postei aqui no Blog <a href="http://brunocarvalho.net/blog/2018/04/03/webinar-explorando-o-ceph/">Explorando o Ceph</a></p>

<p>Podemos comparar o Scrubbing do Ceph com o fsck para o armazenamento de objetos dentro do Cluster.
Ao executar “ceph -s” na ultimas linhas vemos as “placement groups(PGs)” que estão “active+clean”
Para cada placement groups(PGs) o Ceph gera um catálogo de todos os objetos e compara cada objeto principal e suas réplicas para garantir que nenhum objeto esteja ausente ou seja incompatível.
Muitas vezes vamos achar a seguinte informação “1 active+clean+inconsistent” isso nos mostrar que alguma replicar dentro do cluster está inconsistente.</p>

<p>A passagem do Scrubbing é penosa para o cluster e muitas vezes ele só passará quando o sistema estiver com uma carga menor ou se as configurações forem alteradas via inject nas “OSDs”.</p>

<p><strong>Verificando configurações no Ceph:</strong></p>

<p>Apresenta apenas as configurações default do ceph</p>

<pre><code># ceph --show-config 
</code></pre>

<p>Apresenta a configuração permanente do ceph.conf</p>

<pre><code># ceph -n osd.X --show-config
</code></pre>

<p>Apresentar a configuração atual</p>

<pre><code># ceph --admin-daemon /var/run/ceph/ceph-osd.21.asok config show 
</code></pre>

<p>Alterando Configuração a quente</p>

<pre><code># ceph tell osd.X injectargs '--osd_scrub_load_threshold 0.5'
</code></pre>

<p>O Scrub é muito importante para manter a integridade dos dados, mas poderá reduzir o desempenho do seu cluster se não for realizado ajustes nas configurações.</p>

<p>Existe dois tipos de Scrub, um e o “Light scrubbing” verifica o tamanho e os atributos do objeto e passa todos os dias e não sobrecarrega tanto seu cluster. O “deep-scrubbing”  e uma limpeza mais profunda  lê os dados e usa somas de verificação para garantir a integridade, ele que passa no decorrer da semana e gera uma carga maior no cluster. Como o “Ceph -s“ conseguimos verificar em quantas “PGs” está executando scrubbing no momento.</p>

<blockquote><p>26 active+clean+scrubbing+deep
7 active+clean+scrubbing</p></blockquote>

<p>Executando o comando abaixo, você consegue ver quanto tempo passou o último scrub/deep-scrub na pg</p>

<pre><code># Ceph osd PG_ID query 
..................
            "last_deep_scrub_stamp": "2018-08-23 15:48:42.107928",
..................   
</code></pre>

<p>Algumas configurações que devemos nos atentar:</p>

<ul>
<li>osd_scrub_min_interval</li>
<li>osd_scrub_max_interval</li>
<li>osd_scrub_load_threshold</li>
<li>osd_max_scrubs</li>
<li>osd_deep_scrub_interval</li>
<li>osd_scrub_interval_randomize_ratio</li>
<li>osd_scrub_during_recovery</li>
</ul>


<p>Os Scrubs começam após osd_scrub_min_interval desde o último scrub ter passado, isso só ocorre se a CPU estiver abaixo do osd_scrub_load_threshold ou após osd_scrub_max_interval mesmo se o sistema estiver sobrecarregado.</p>

<p>Podemos também configurar um intervalo na madrugada para o scrub passar osd_scrub_begin_hour/osd_scrub_end_hour, com lançamento do Mimic temos mais duas opções de intervalos osd_scrub_begin_week_day/osd_scrub_end_week_day</p>

<p>Quando executamos um ceph pg repair, ou um ceph pg deep-scrub ele não irá executar imediatamente e sim enviará um pedido a pg e o osd primario “instructing pg ID on osd.X to repair” porem precisa satisfazer as regras configuradas.</p>

<blockquote><p>Nota: “Ceph will not scrub when the system load (as defined by getloadavg() / number of onlinecpus) is higher than this number. Default is 0.5.”</p></blockquote>

<p>A passagem deep-scrub não necessariamente depende do load_threshold</p>

<blockquote><p>Nota: “The interval for “deep” scrubbing (fully reading all data). The osd scrub load threshold does not affect this setting.”</p></blockquote>

<p>Atenção as frags noscrub e nodeep-scrub definida no cluster e nas pools, caso esteja setada o scrub não será executado.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Webinar Explorando o Ceph]]></title>
    <link href="http://brunocarvalho.net/blog/2018/04/03/webinar-explorando-o-ceph/"/>
    <updated>2018-04-03T14:27:42-03:00</updated>
    <id>http://brunocarvalho.net/blog/2018/04/03/webinar-explorando-o-ceph</id>
    <content type="html"><![CDATA[<p><img src="/images/ceph/webinartivit.jpg" alt="" /></p>

<p>A Internet das Coisas e a Terceira Plataforma Computacional estão levando as pessoas e as máquinas a gerarem milhões de dados digitais a cada instante. O volume de informações criadas no mundo, a médio e longo prazo, é infinito. Pesquisa da IBM mostra que, até 2020, o volume de dados previsto é de 40 zetabytes, ou seja, haverá quatro vezes mais dados digitais do que todos os grãos de areia existentes no planeta.  As máquinas irão gerar 42% de todo volume de dados e haverá mais de 200 bilhões de dispositivos conectados no mundo.</p>

<p>O crescimento desenfreado de dados e as constantes mudanças exige das empresas respostas rápidas para os negócios. A expansão dos dispositivos móveis aumentou o uso das redes sociais e também de dados. As companhias precisam de tecnologias para tratar dados com estratégias de Big Data e gerar mais serviços online para o mundo conectado.</p>

<p>Atualmente as empresas precisam ter serviços disponíveis 24 horas, a Cloud Computing abraça todas as aplicações criadas para abastecer o mundo conectado, tanto para o mercado corporativo quando para atender o consumidor final. Para que isso gere menos custo e valor para empresas precisamos de um mundo com soluções hiperconvergentes, e sem um storage eficiente e escalável baseado em Software Defined isso se tornaria inviável para muitas empresas e startups.</p>

<p>No Webinar “Explorando Ceph” que será realizado pela TIVIT na próxima quarta-feira dia 4, falarei um pouco mais sobre o storage que está revolucionando o mercado de Software Defined, será totalmente gratuito e online. Inscreva-se: <a href="http://go.tivit.com/webinar-explorando-ceph">Aqui</a></p>

<p><strong>GRAVAÇÃO:</strong></p>

<iframe width="100%" height="350px" src="https://www.youtube.com/embed/DayGJbPtB04" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Um pouco sobre o Ceph Day SP 2017]]></title>
    <link href="http://brunocarvalho.net/blog/2017/10/27/um-pouco-sobre-o-ceph-day-sp-2017/"/>
    <updated>2017-10-27T11:09:02-03:00</updated>
    <id>http://brunocarvalho.net/blog/2017/10/27/um-pouco-sobre-o-ceph-day-sp-2017</id>
    <content type="html"><![CDATA[<p><img src="/images/ceph/cephday0.png" alt="" /></p>

<p>Na última Quarta-feira(25/10) rolou o primeiro Ceph Day SP e o primeiro na América Latina organizado e realizado pelo nosso Manager Comunity Leonado Vaz e nosso amigo da Savant Marcos Sungaila, além de grandes patrocinadores como RedHat, Canonical, Suse entre outros&hellip;</p>

<p>Para quem não teve a oportunidade de estar presente perdeu um grande dia. Discutimos sobre a nova versão do Ceph, custos x performance, implementações, cases e principalmente o futuro do SDS</p>

<p><img src="/images/ceph/cephday1.png" alt="" /></p>

<p>A chegada da nova versão Luminous LTS que já é uma realidade, virou um divisor de águas no mundo do Software Define Storage(SDS) com várias melhorias como o novo backend BlueStorage que leva o Ceph a um nível de performance 2x mais rápido eliminando a camada do filesystem, além de grandes melhorias em toda sua estrutura.</p>

<p>Precisando de baixa latência e alta performance? O Ceph te entrega, tivemos uma palestra da Intel, uma das grandes colaboradoras do código, falando sobre Ceph com All-Flash.</p>

<p><a href="https://software.intel.com/en-us/articles/using-intel-optane-and-intel-3d-nand-technology-with-ceph-to-build-high-performance-cloud">SSDs to Build High-Performance Cloud Storage Solutions</a></p>

<p>Para os amantes do Zabbix como eu, o novo serviço de gerenciamento do Ceph(ceph-mgr) foi ampliado com vários módulos python, um deles exporta o status geral do cluster para o Zabbix habilitando o modulo no Ceph e configurando o template no zabbix.</p>

<pre><code class="bash">$ ceph zabbix config-set zabbix_host zabbix-server.local
$ ceph zabbix config-set identifier ceph.local
</code></pre>

<p>Para os amantes da interface GUI. Agora será possível ter um dashboard no Ceph de forma simples.</p>

<pre><code class="bash">$ ceph mgr module enable dashboard
</code></pre>

<p>Acesse <a href="http://you_active_mgr_host:7000/">http://you_active_mgr_host:7000/</a></p>

<p><img src="/images/ceph/cephday2.png" alt="" />
<img src="/images/ceph/cephday3.png" alt="" /></p>

<p>Outra grande novidade, agora já é possível fazer Erasure Coding tanto em RBD quanto em CephFS, que antes só era possível com object. Na mudança para o BlueStorage será possível suporta compressão de dados, trazendo mais economia no armazenamento.</p>

<p>E tem muito mais vindo por ai! Eu particularmente estou ansioso para o tão esperado QoS que está por vir na próxima versão ou dedup que será excelente para galera de backup. Aguardem que isso será só o começo da evolução que o Ceph vem trazendo para o mundo do Software Define Storage.</p>

<p>Referências:</p>

<p><a href="http://ceph.com/releases/v12-2-0-luminous-released/">http://ceph.com/releases/v12-2-0-luminous-released/</a></p>

<p><a href="http://ceph.com/community/new-luminous-dashboard/">http://ceph.com/community/new-luminous-dashboard/</a></p>

<p><a href="http://ceph.com/community/new-luminous-zabbix/">http://ceph.com/community/new-luminous-zabbix/</a></p>
]]></content>
  </entry>
  
</feed>
