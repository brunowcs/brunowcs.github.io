<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sds | brunocarvalho.net]]></title>
  <link href="http://brunocarvalho.net/blog/categories/sds/atom.xml" rel="self"/>
  <link href="http://brunocarvalho.net/"/>
  <updated>2018-09-12T17:04:07-03:00</updated>
  <id>http://brunocarvalho.net/</id>
  <author>
    <name><![CDATA[Bruno Carvalho]]></name>
    <email><![CDATA[brunowcs@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ceph Scrubbing]]></title>
    <link href="http://brunocarvalho.net/blog/2018/09/11/ceph-scrubbing/"/>
    <updated>2018-09-11T19:03:07-03:00</updated>
    <id>http://brunocarvalho.net/blog/2018/09/11/ceph-scrubbing</id>
    <content type="html"><![CDATA[<p><span style="display:block;text-align:center"><img src="/images/ceph/ceph.png" alt="" /> </span></p>

<p>Olá turmadaaa, um pouco sumido, mas estou de volta com grandes novidades. A pouco mais de 2 anos realizei a implantação de 4 Cluster Ceph com Juju e MaaS na america latina que hoje tem quase 8 Petabyte raw, e quanto mais o cluster cresce, mais ajustes e problemas diferentes aparecem.</p>

<p>Recentemente encontrei um bug relacionado a recuperação de “Placement groups(PGs)” dentro do Jewel. O Ceph não estava conseguindo resolver a inconsistência de uma PG, mesmo executando o “ceph repair” e forçando a passagem do Scrubbing. Pelo que notei nos últimos lançamentos do CEPH tem alguns bug fix sobre o assunto.</p>

<p>Bem este será o primeiro de alguns posts mais avançado que irei falar de como resolver manualmente uma pg inconsistente, mas neste momento vamos entender melhor o Scrubbing no Ceph</p>

<p>Podemos comparar o Scrubbing do Ceph com o fsck para o armazenamento de objetos dentro do Cluster.
Ao executar “ceph -s” nas ultimas linhas veremos as “placement groups(PGs)” que estão como “active+clean”, que significa que estão oks.</p>

<p>Para cada placement groups(PGs) o Ceph gera um catálogo de todos os objetos e compara cada objeto primário e suas réplicas garantindo assim que nenhum objeto esteja ausente ou seja incompatível. Muitas vezes vamos achar a seguinte informação “1 active+clean+inconsistent” isso nos mostrar que alguma replicar dentro do cluster está inconsistente. Como o Ceph realizado seu auto reconvery esse problema é resolvido automaticamente na passagem do scrub, porem recentemente percebi que isso não é uma verdade absoluta se você não tiver um ambiente bem configurado.</p>

<p>A passagem do Scrubbing é penosa para o cluster e muitas vezes só passará quando o sistema estiver com uma carga menor ou se as configurações forem alteradas via inject nas “OSDs”. Veja as 3 formas que podemos verificar as configurações no cluster e como podemos alterar a quente e força uma passagem imediata do scrub.</p>

<p><strong>Verificando configurações no Ceph:</strong></p>

<p>Apresenta apenas as configurações default do ceph</p>

<pre><code># ceph --show-config | grep osd_scrub_load_threshold
</code></pre>

<p>Apresenta a configuração permanente do ceph.conf</p>

<pre><code># ceph -n osd.X --show-config | grep osd_scrub_load_threshold
</code></pre>

<p>Apresentar a configuração atual</p>

<pre><code># ceph --admin-daemon /var/run/ceph/ceph-osd.21.asok config show | grep osd_scrub_load_threshold
</code></pre>

<p>Alterando Configuração a quente</p>

<pre><code># ceph tell osd.X injectargs '--osd_scrub_load_threshold 0.5' 
</code></pre>

<p>O Scrubbing é muito importante para manter a integridade dos dados, mas poderá reduzir o desempenho do seu cluster, como &ldquo;requests are blocked&rdquo; se não for realizado ajustes nas configurações baseado na carga do seu cluster.</p>

<p>Existe dois tipos de Scrubbing, um e o “Light scrubbing”, ele verifica o tamanho e os atributos do objeto e por default passa todos os dias e não sobrecarrega tanto o cluster. O “deep-scrubbing”  e uma limpeza mais profunda, lê os dados e usa somas de verificação para garantir a integridade, esse Scrubbing passa no decorrer da semana e gera uma carga maior no cluster. Como o “ceph -s“ conseguimos verificar em quantas “PGs” está sendo executado scrubbing no momento.</p>

<blockquote><p>26 active+clean+scrubbing+deep
7 active+clean+scrubbing</p></blockquote>

<p>Executando o comando abaixo, você consegue ver quando passou o último scrub/deep-scrub na PG</p>

<pre><code># Ceph osd PG_ID query 
..................
            "last_deep_scrub_stamp": "2018-08-23 15:48:42.107928",
..................   
</code></pre>

<p>Algumas configurações que devemos nos atentar para ajustar:</p>

<ul>
<li>osd_scrub_min_interval</li>
<li>osd_scrub_max_interval</li>
<li>osd_scrub_load_threshold</li>
<li>osd_max_scrubs</li>
<li>osd_deep_scrub_interval</li>
<li>osd_scrub_interval_randomize_ratio</li>
<li>osd_scrub_during_recovery</li>
</ul>


<p>Os Scrubbing começam após osd_scrub_min_interval desda ultima passagem, isso só ocorre se a CPU estiver abaixo do osd_scrub_load_threshold ou após osd_scrub_max_interval mesmo se o sistema estiver sobrecarregado.</p>

<p>Podemos também configurar um intervalo na madrugada para o Scrubbing passar osd_scrub_begin_hour/osd_scrub_end_hour. Com lançamento do Mimic, temos mais duas opções de intervalos osd_scrub_begin_week_day/osd_scrub_end_week_day</p>

<p>Quando executamos um ceph pg repair, ou um ceph pg deep-scrub ele não irá executar imediatamente e sim enviará um pedido a pg “instructing pg ID on osd.X to repair” porem precisa satisfazer as regras configuradas, veja a nota da documentação oficial abaixo.</p>

<blockquote><p>Nota: “Ceph will not scrub when the system load (as defined by getloadavg() / number of onlinecpus) is higher than this number. Default is 0.5.”</p></blockquote>

<p>A passagem deep-scrub não necessariamente depende do load_threshold, veja a nota abaixo.</p>

<blockquote><p>Nota: “The interval for “deep” scrubbing (fully reading all data). The osd scrub load threshold does not affect this setting.”</p></blockquote>

<p>Atenção as frags noscrub e nodeep-scrub definida no cluster e nas pools, caso esteja setada o scrub não será executado.</p>

<p>Creio que com essas informações já conseguimos manipular melhor os Scrubbing no cluster, que e essencial para a consistência das  “Placement groups(PGs)” e da performance do cluster. Nos próximos posts vamos entrar mais a fundo na manipulação manual do objeto e na sua recuperação.</p>

<p> Caso tenha alguma dúvida sobre o funcionamento do Ceph não deixa de assitir meu Webinar que postei aqui no Blog <a href="http://brunocarvalho.net/blog/2018/04/03/webinar-explorando-o-ceph/">Explorando o Ceph</a></p>

<p> Como tiveram várias dúvidas sobre o Webinar, lançarei uma série de artigos mais básico com informações essencias para você ir “Explorando o Ceph”.</p>

<p>Nos próximo posts irei apresentar:</p>

<p><a href="http://brunocarvalho.net/"> - Resolvendo PGs inconsistent no CEPH manualmente </a></p>

<p>Referencia: <a href="http://docs.ceph.com/docs/master/rados/configuration/osd-config-ref/">http://docs.ceph.com/docs/master/rados/configuration/osd-config-ref/</a></p>
]]></content>
  </entry>
  
</feed>
