<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[brunocarvalho.net]]></title>
  <link href="http://brunocarvalho.net/atom.xml" rel="self"/>
  <link href="http://brunocarvalho.net/"/>
  <updated>2016-11-22T19:17:25-02:00</updated>
  <id>http://brunocarvalho.net/</id>
  <author>
    <name><![CDATA[Bruno Carvalho]]></name>
    <email><![CDATA[brunowcs@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Instalando Ovirt 4.0 Hosted Engine utilizando NFS]]></title>
    <link href="http://brunocarvalho.net/blog/2016/11/21/instalando-ovirt-4-dot-0-hosted-engine-utilizando-nfs/"/>
    <updated>2016-11-21T12:07:53-02:00</updated>
    <id>http://brunocarvalho.net/blog/2016/11/21/instalando-ovirt-4-dot-0-hosted-engine-utilizando-nfs</id>
    <content type="html"><![CDATA[<p><img src="http://brunocarvalho.net/images/ovirt/ovirt4.png" alt="" /></p>

<p><strong>Objetivo</strong></p>

<p>Instalar Ovirt 4.0 Hosted Engine utilizando NFS</p>

<p>No meu ambiente instalei tudo em um único node, o ovirt engine como VM no meu primeiro node, juntamente com NFS, mas para ambiente maiores recomendo a separação dos componentes para obter uma melhor performance e HA.</p>

<p><strong>Ambiente Utilizado</strong></p>

<ul>
<li>NODE1 FÍSICO: Intel Xeon 24 Core(Hyper-Threading) - 32G - CentoOS 7-x86_64-minimal</li>
<li>NFS STORAGE - 500GB</li>
<li>VM OVIRT MANAGER - Quad core, 16 GB RAM e 50 GB</li>
<li>SELINUX Desabilitado</li>
<li>NTP configurado</li>
</ul>


<p>Necessário servidor DNS, mas utilizei local para esta instalação</p>

<pre><code># cat /etc/hosts

192.168.10.200  ovirtengine.supcom
192.168.10.201  node1.supcom
</code></pre>

<p><strong>Instando e configurando NFS</strong></p>

<pre><code># yum -y install nfs-utils

# mkdir -p /storage/{engine,data,iso} 

# chown 36:36 /storage/{engine,data,iso}

# cat /etc/exports

/storage/engine 192.168.10.0/24(rw,anonuid=36,anongid=36,all_squash)
/storage/data   192.168.10.0/24(rw,anonuid=36,anongid=36,all_squash)
/storage/iso    192.168.10.0/24(rw,anonuid=36,anongid=36,all_squash)

# exportfs -a

# systemctl enable nfs-server.service
# systemctl restart nfs-server.service

# showmount -e
Export list for node1.supcom:
/storage/engine *
/storage/data   *
/storage/iso*
</code></pre>

<p><strong>Instalando pacotes necessários</strong></p>

<pre><code># yum install http://resources.ovirt.org/pub/yum-repo/ovirt-release40.rpm
# yum install ovirt-hosted-engine-setup ovirt-engine-appliance -y
</code></pre>

<p><strong>Iniciando deploy hosted engine</strong></p>

<pre><code># hosted-engine --deploy
[ INFO  ] Stage: Initializing
[ INFO  ] Generating a temporary VNC password.
[ INFO  ] Stage: Environment setup
  During customization use CTRL-D to abort.
  Continuing will configure this host for serving as hypervisor and create a VM where you have to install the engine afterwards.
</code></pre>

<p>Are you sure you want to continue? (Yes, No)[Yes]: <strong>Yes</strong></p>

<pre><code>  It has been detected that this program is executed through an SSH connection without using screen.
  Continuing with the installation may lead to broken installation if the network connection fails.
  It is highly recommended to abort the installation and run it inside a screen session using command "screen".
</code></pre>

<p>Do you want to continue anyway? (Yes, No)[No]: <strong>Yes</strong></p>

<pre><code>[ INFO  ] Hardware supports virtualization
  Configuration files: []
  Log file: /var/log/ovirt-hosted-engine-setup/ovirt-hosted-engine-setup-20161118133413-u1wg2v.log
  Version: otopi-1.5.2 (otopi-1.5.2-1.el7.centos)
[ INFO  ] Stage: Environment packages setup
[ INFO  ] Stage: Programs detection
[ INFO  ] Stage: Environment setup
[WARNING] Cannot locate gluster packages, Hyper Converged setup support will be disabled.
[ INFO  ] Please abort the setup and install vdsm-gluster, gluster-server &gt;= 3.7.2 and restart vdsmd service in order to gain Hyper Converged setup support.
[ INFO  ] Stage: Environment customization

  --== STORAGE CONFIGURATION ==--
</code></pre>

<p>Please specify the storage you would like to use (glusterfs, iscsi, fc, nfs3, nfs4)[nfs3]: <strong>nfs3</strong></p>

<p>Please specify the full shared storage connection path to use (example: host:/path): <strong>192.168.10.201:/storage/engine</strong></p>

<pre><code>[ INFO  ] Installing on first host

  --== SYSTEM CONFIGURATION ==--


  --== NETWORK CONFIGURATION ==--

[ INFO  ] Bridge ovirtmgmt already created
</code></pre>

<p>iptables was detected on your computer, do you wish setup to configure it? (Yes, No)[Yes]: <strong>Yes</strong></p>

<p>Please indicate a pingable gateway IP address [192.168.10.1]: <strong>192.168.10.1</strong></p>

<pre><code> --== VM CONFIGURATION ==--

      Booting from cdrom on RHEL7 is ISO image based only, as cdrom passthrough is disabled (BZ760885)
</code></pre>

<p>Please specify the device to boot the VM from (choose disk for the oVirt engine appliance)
(cdrom, disk, pxe) [disk]: <strong>disk</strong></p>

<p>Please specify the console type you would like to use to connect to the VM (vnc, spice) [vnc]: <strong>vnc</strong></p>

<pre><code>[ INFO  ] Detecting available oVirt engine appliances
  The following appliance have been found on your system:
[1] - The oVirt Engine Appliance image (OVA) - 4.0-20161115.1.el7.centos
[2] - Directly select an OVA file
</code></pre>

<p>Please select an appliance (1, 2) [1]: <strong>1</strong></p>

<pre><code>[ INFO  ] Verifying its sha1sum
[ INFO  ] Checking OVF archive content (could take a few minutes depending on archive size)
[ INFO  ] Checking OVF XML content (could take a few minutes depending on archive size)
[WARNING] OVF does not contain a valid image description, using default.
</code></pre>

<p>Would you like to use cloud-init to customize the appliance on the first boot (Yes, No)[Yes]? <strong>Yes</strong></p>

<p>Would you like to generate on-fly a cloud-init ISO image (of no-cloud type)
or do you have an existing one (Generate, Existing)[Generate]? <strong>Generate</strong></p>

<pre><code>      Please provide the FQDN you would like to use for the engine appliance.
      Note: This will be the FQDN of the engine VM you are now going to launch,
      it should not point to the base host or to any other existing machine.
</code></pre>

<p>Engine VM FQDN: (leave it empty to skip):  []: <strong>ovirtengine.supcom</strong></p>

<p>Automatically execute engine-setup on the engine appliance on first boot (Yes, No)[Yes]? <strong>No</strong></p>

<pre><code>    Please provide the domain name you would like to use for the engine appliance.
    Engine VM domain: [supcom]

      Enter root password that will be used for the engine appliance (leave it empty to skip):
      Confirm appliance root password:
      The following CPU types are supported by this host:
             - model_SandyBridge: Intel SandyBridge Family
             - model_Westmere: Intel Westmere Family
             - model_Nehalem: Intel Nehalem Family
             - model_Penryn: Intel Penryn Family
             - model_Conroe: Intel Conroe Family
      Please specify the CPU type to be used by the VM [model_SandyBridge]:
      Please specify the number of virtual CPUs for the VM (Defaults to appliance OVF value): [4]:
      [WARNING] Minimum requirements for disk size not met
      You may specify a unicast MAC address for the VM or accept a randomly generated default [00:16:3e:77:82:77]:
      Please specify the memory size of the VM in MB (Defaults to appliance OVF value): [16384]:
</code></pre>

<p>How should the engine VM network be configured (DHCP, Static)[DHCP]? Static
Please enter the IP address to be used for the engine VM [192.168.10.2]: <strong>192.168.10.200</strong></p>

<pre><code>      [ INFO  ] The engine VM will be configured to use 192.168.10.200/24
      Please provide a comma-separated list (max 3) of IP addresses of domain name servers for the engine VM
</code></pre>

<p>Engine VM DNS (leave it empty to skip) [8.8.8.8]: <strong>8.8.8.8</strong></p>

<pre><code>      Add lines for the appliance itself and for this host to /etc/hosts on the engine VM?
</code></pre>

<p>Note: ensuring that this host could resolve the engine VM hostname is still up to you
(Yes, No)[No] <strong>Yes</strong></p>

<pre><code>      --== HOSTED ENGINE CONFIGURATION ==--

      Enter engine admin password:
      Confirm engine admin password:
      Enter the name which will be used to identify this host inside the Administrator Portal [hosted_engine_1]: node1-engine
      Please provide the name of the SMTP server through which we will send notifications [localhost]:
      Please provide the TCP port number of the SMTP server [25]:
      Please provide the email address from which notifications will be sent [root@localhost]:
      Please provide a comma-separated list of email addresses which will get notifications [root@localhost]:
        [ INFO  ] Stage: Setup validation

      --== CONFIGURATION PREVIEW ==--

      Bridge interface                   : em2
      Engine FQDN                        : ovirtengine.supcom
      Bridge name                        : ovirtmgmt
      Host address                       : node1.supcom
      SSH daemon port                    : 22
      Firewall manager                   : iptables
      Gateway address                    : 192.168.10.1
      Host name for web application      : node1-engine
      Storage Domain type                : nfs3
      Host ID                            : 1
      Image size GB                      : 10
      Storage connection                 : 192.168.10.201:/storage/engine
      Console type                       : vnc
      Memory size MB                     : 16384
      MAC address                        : 00:16:3e:17:ac:23
      Boot type                          : disk
      Number of CPUs                     : 4
      OVF archive (for disk boot)        : /usr/share/ovirt-engine-appliance/ovirt-engine-appliance-4.0-20161115.1.el7.centos.ova
      Appliance version                  : 4.0-20161115.1.el7.centos
      Restart engine VM after engine-setup: True
      CPU Type                           : model_SandyBridge
</code></pre>

<p>Acima o resumo da minha configuração, mude o que achar necessário conforme seu ambiente.</p>

<p>Please confirm installation settings (Yes, No)[Yes]: <strong>Yes</strong></p>

<pre><code>    [ INFO  ] Stage: Transaction setup
    [ INFO  ] Stage: Misc configuration
    [ INFO  ] Stage: Package installation
    [ INFO  ] Stage: Misc configuration
    [ INFO  ] Configuring libvirt
    [ INFO  ] Configuring VDSM
    [ INFO  ] Starting vdsmd
    [ INFO  ] Waiting for VDSM to reply
    [ INFO  ] Creating Storage Domain
    [ INFO  ] Creating Storage Pool
    [ INFO  ] Connecting Storage Pool
    [ INFO  ] Verifying sanlock lockspace initialization
    [ INFO  ] Creating Image for 'hosted-engine.lockspace' ...
    [ INFO  ] Image for 'hosted-engine.lockspace' created successfully
    [ INFO  ] Creating Image for 'hosted-engine.metadata' ...
    [ INFO  ] Image for 'hosted-engine.metadata' created successfully
    [ INFO  ] Creating VM Image
    [ INFO  ] Extracting disk image from OVF archive (could take a few minutes depending on archive size)
    [ INFO  ] Validating pre-allocated volume size
    [ INFO  ] Uploading volume to data domain (could take a few minutes depending on archive size)
    [ INFO  ] Image successfully imported from OVF
    [ INFO  ] Destroying Storage Pool
    [ INFO  ] Start monitoring domain
    [ INFO  ] Configuring VM
    [ INFO  ] Updating hosted-engine configuration
    [ INFO  ] Stage: Transaction commit
    [ INFO  ] Stage: Closing up
    [ INFO  ] Creating VM
      You can now connect to the VM with the following command:
            hosted-engine --console
      You can also graphically connect to the VM from your system with the following command:
            remote-viewer vnc://node1.supcom:5900
      Use temporary password "1485XCFX" to connect to vnc console.
      Please ensure that your Guest OS is properly configured to support serial console according to your distro documentation.
      Follow http://www.ovirt.org/Serial_Console_Setup#I_need_to_access_the_console_the_old_way for more info.
      If you need to reboot the VM you will need to start it manually using the command:
      hosted-engine --vm-start
      You can then set a temporary password using the command:
      hosted-engine --add-console-password
      Please install and setup the engine in the VM.
      You may also be interested in installing ovirt-guest-agent-common package in the VM.


      The VM has been rebooted.
      To continue please install oVirt-Engine in the VM
      (Follow http://www.ovirt.org/Quick_Start_Guide for more info).

      Make a selection from the options below:
      (1) Continue setup - oVirt-Engine installation is ready and ovirt-engine service is up
      (2) Abort setup
      (3) Power off and restart the VM
      (4) Destroy VM and abort setup

      (1, 2, 3, 4)[1]:
</code></pre>

<p>A partir desse ponto você precisar conectar na VM via VNC, para inciar o setup da engine.</p>

<p>Na minha instalação eu utilizei o Tightvnc (<a href="http://www.tightvnc.com/download.php">http://www.tightvnc.com/download.php</a>)</p>

<p>Após acessar via vnc a maquina virtual, execute o seguinte comando</p>

<pre><code>[root@ovirtengine ~]# engine-setup
[ INFO  ] Stage: Initializing
[ INFO  ] Stage: Environment setup
  Configuration files: ['/etc/ovirt-engine-setup.conf.d/10-packaging-jboss.conf', '/etc/ovirt-engine-setup.conf.d/10-packaging.conf']
  Log file: /var/log/ovirt-engine/setup/ovirt-engine-setup-20161118191609-oxmvpo.log
  Version: otopi-1.5.2 (otopi-1.5.2-1.el7.centos)
[ INFO  ] Stage: Environment packages setup

......................
......................

--== END OF CONFIGURATION ==--

[ INFO  ] Stage: Setup validation
[WARNING] Cannot validate host name settings, reason: cannot resolve own name 'ovirtengine'

      --== CONFIGURATION PREVIEW ==--

      Application mode                        : both
      Default SAN wipe after delete           : False
      Firewall manager                        : firewalld
      Update Firewall                         : True
      Host FQDN                               : ovirtengine.supcom
      Engine database secured connection      : False
      Engine database host                    : localhost
      Engine database user name               : engine
      Engine database name                    : engine
      Engine database port                    : 5432
      Engine database host name validation    : False
      DWH database secured connection         : False
      DWH database host                       : localhost
      DWH database user name                  : ovirt_engine_history
      DWH database name                       : ovirt_engine_history
      DWH database port                       : 5432
      DWH database host name validation       : False
      Engine installation                     : True
      NFS setup                               : False
      PKI organization                        : supcom
      Configure local Engine database         : True
      Set application as default page         : True
      Configure Apache SSL                    : True
      DWH installation                        : True
      Configure local DWH database            : True
      Engine Host FQDN                        : ovirtengine.supcom
      Configure Image I/O Proxy               : True
      Configure VMConsole Proxy               : True
      Configure WebSocket Proxy               : True

      Please confirm installation settings (OK, Cancel) [OK]: OK
</code></pre>

<p>Retirei parte do log padrão para não ficar muito grande o post, acima segue o resumo da minha configuração. Visto que todo ambiente será instalado nessa vm, tanto banco de dados como o DWH, então basta confirmar e seguir a configuração padrão.</p>

<pre><code>[ INFO  ] Stage: Transaction setup
[ INFO  ] Stopping engine service
[ INFO  ] Stopping ovirt-fence-kdump-listener service
[ INFO  ] Stopping dwh service
[ INFO  ] Stopping Image I/O Proxy service
[ INFO  ] Stopping websocket-proxy service
[ INFO  ] Stage: Misc configuration
[ INFO  ] Stage: Package installation
[ INFO  ] Stage: Misc configuration
[ INFO  ] Upgrading CA
[ INFO  ] Initializing PostgreSQL
[ INFO  ] Creating PostgreSQL 'engine' database
[ INFO  ] Configuring PostgreSQL
[ INFO  ] Creating PostgreSQL 'ovirt_engine_history' database
[ INFO  ] Configuring PostgreSQL
[ INFO  ] Creating CA
[ INFO  ] Creating/refreshing Engine database schema
[ INFO  ] Creating/refreshing DWH database schema
[ INFO  ] Configuring Image I/O Proxy
[ INFO  ] Setting up ovirt-vmconsole proxy helper PKI artifacts
[ INFO  ] Setting up ovirt-vmconsole SSH PKI artifacts
[ INFO  ] Configuring WebSocket Proxy
[ INFO  ] Creating/refreshing Engine 'internal' domain database schema
[ INFO  ] Generating post install configuration file '/etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf'
[ INFO  ] Stage: Transaction commit
[ INFO  ] Stage: Closing up
[ INFO  ] Starting engine service
[ INFO  ] Restarting nfs services
[ INFO  ] Starting dwh service
[ INFO  ] Restarting ovirt-vmconsole proxy service

  --== SUMMARY ==--

[ INFO  ] Restarting httpd
  Please use the user 'admin@internal' and password specified in order to login
  Web access is enabled at:
  http://ovirtengine.supcom:80/ovirt-engine
  https://ovirtengine.supcom:443/ovirt-engine
  Internal CA 81:C1:01:AD:28:0E:46:8F:86:A8:BB:12:E9:BB:2B:5F:0E:8C:F0:8D
  SSH fingerprint: d7:7b:83:5b:54:6e:46:7b:c1:2c:c0:6e:ae:a1:4c:1e

  --== END OF SUMMARY ==--

[ INFO  ] Stage: Clean up
  Log file is located at /var/log/ovirt-engine/setup/ovirt-engine-setup-20161118192015-muij8n.log
[ INFO  ] Generating answer file '/var/lib/ovirt-engine/setup/answers/20161118192638-setup.conf'
[ INFO  ] Stage: Pre-termination
[ INFO  ] Stage: Termination
[ INFO  ] Execution of setup completed successfully
</code></pre>

<p>Após a finalização, vamos voltar para o node1 e selecionar a primeira opção, para finalizar.</p>

<pre><code>Make a selection from the options below:
  (1) Continue setup - oVirt-Engine installation is ready and ovirt-engine service is up
  (2) Abort setup
  (3) Power off and restart the VM
  (4) Destroy VM and abort setup

  (1, 2, 3, 4)[1]: 1

Checking for oVirt-Engine status at ovirtengine.supcom...
[ INFO  ] Engine replied: DB Up!Welcome to Health Status!
[ INFO  ] Acquiring internal CA cert from the engine
[ INFO  ] The following CA certificate is going to be used, please immediately interrupt if not correct:
[ INFO  ] Issuer: C=US, O=supcom, CN=ovirtengine.supcom.60122, Subject: C=US, O=supcom, CN=ovirtengine.supcom.60122, Fingerprint (SHA-1): 81C101AD280E468F86A8BB12E9BB2B5F0E8CF08D
[ INFO  ] Connecting to the Engine
[ INFO  ] Waiting for the host to become operational in the engine. This may take several minutes...
[ INFO  ] Still waiting for VDSM host to become operational...
[ INFO  ] The VDSM Host is now operational
[ INFO  ] Saving hosted-engine configuration on the shared storage domain

  Please shutdown the VM allowing the system to launch it as a monitored service.
  The system will wait until the VM is down.
</code></pre>

<p>Agora vamos voltar ao ovirtengine e reiniciar a vm.</p>

<pre><code>[root@ovirtengine ~]# shutdown -r now
</code></pre>

<p>Voltando ao node1, percebemos que a instalação finalizou com sucesso.</p>

<pre><code>[ INFO  ] Enabling and starting HA services
[ INFO  ] Stage: Clean up
[ INFO  ] Generating answer file '/var/lib/ovirt-hosted-engine-setup/answers/answers-20161118174208.conf'
[ INFO  ] Generating answer file '/etc/ovirt-hosted-engine/answers.conf'
[ INFO  ] Stage: Pre-termination
[ INFO  ] Stage: Termination
[ INFO  ] Hosted Engine successfully deployed
[root@node1 ~]#
</code></pre>

<p>Vamos agora iniciar o hosted-engine</p>

<pre><code>[root@node1 ~]# hosted-engine --vm-start
</code></pre>

<p>Caso a vm ovirtengine não inicie, restart os serviços</p>

<pre><code>[root@node1 ~]# systemctl restart ovirt-ha-agent &amp;&amp; systemctl restart ovirt-ha-broker &amp;&amp; systemctl restart vdsmd
[root@node1 ~]# hosted-engine --vm-start
[root@node1 ~]# hosted-engine --vm-status

--== Host 1 status ==--

Status up-to-date  : True
Hostname   : node1.supcom
Host ID: 1
Engine status  : {"health": "good", "vm": "up", "detail": "up"}
Score  : 3400
stopped: False
Local maintenance  : False
crc32  : f322c3c1
Host timestamp : 235828
Extra metadata (valid at timestamp):
metadata_parse_version=1
metadata_feature_version=1
timestamp=235828 (Mon Nov 21 13:29:31 2016)
host-id=1
score=3400
maintenance=False
state=EngineUp
stopped=False
</code></pre>

<p>Agora vá para browser e acesse o portal de administração</p>

<pre><code>https://ovirtengine.supcom/ovirt-engine/
</code></pre>

<p>login:admin</p>

<p>senha: que você configurou na instalação</p>

<p><img src="http://brunocarvalho.net/images/ovirt/ovirtlogin.jpg" alt="" /></p>

<p><strong>Adicionando Storage Data</strong></p>

<p>Sistema -> Data Centers -> Default -> Storage -> Novo Dominio”:</p>

<p><img src="http://brunocarvalho.net/images/ovirt/addstorage.jpg" alt="" /></p>

<p><strong>Adicionando Storage ISO</strong></p>

<p><img src="http://brunocarvalho.net/images/ovirt/addstorageiso.jpg" alt="" /></p>

<p><img src="http://brunocarvalho.net/images/ovirt/storageisoaguardando.jpg" alt="" /></p>

<p>Agora basta aguarda o storage ficar ativo.</p>

<p><img src="http://brunocarvalho.net/images/ovirt/storageisook.jpg" alt="" /></p>

<p><strong>Adicionando imagem ISO no Dominio criado</strong></p>

<p>Acesse o ovirtengine e digite os comandos abaixo</p>

<pre><code>[root@ovirtengine ~]# wget http://mirror.globo.com/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1511.iso

[root@ovirtengine ~]# engine-iso-uploader upload -i node1-iso CentOS-7-x86_64-Minimal-1511.iso -r ovirtengine.supcom --insecure
Please provide the REST API password for the admin@internal oVirt Engine user (CTRL+D to abort):
Uploading, please wait...
INFO: Start uploading CentOS-7-x86_64-Minimal-1511.iso
Uploading: [########################################] 100%
INFO: CentOS-7-x86_64-Minimal-1511.iso uploaded successfully
</code></pre>

<blockquote><p>Caso receba nfs timeout verifique se o firewall do node1.supcom não está bloqueando.</p></blockquote>

<p><strong>Criando VM</strong></p>

<p>Sistema -> Data Centers -> Default -> Cluster -> Default - > MVS -> Novo MV”:</p>

<p>Após digitar o nome da VM,  vamos criar o disco da VM em &ldquo;Imagens de Instâncias&rdquo; click em Criar</p>

<p><img src="http://brunocarvalho.net/images/ovirt/novavm.jpg" alt="" /></p>

<p>Digite o Tamanho, verifique se o storage domain está correto e click em OK</p>

<p><img src="http://brunocarvalho.net/images/ovirt/createdisk.jpg" alt="" /></p>

<p>Selecione a rede que foi configurada por padrão na instalação.(bridge ovirtmgmt)</p>

<p><img src="http://brunocarvalho.net/images/ovirt/confrede.jpg" alt="" /></p>

<p>Configure a memória e o CPU e click em OK.</p>

<p><img src="http://brunocarvalho.net/images/ovirt/vmmemoria.jpg" alt="" /></p>

<p>Agora vamos iniciar a VM  clicando em &ldquo;Executar uma vez&rdquo; para iniciar o sistema com boot da nossa ISO CentOS</p>

<p><img src="http://brunocarvalho.net/images/ovirt/iniciarvm.jpg" alt="" /></p>

<p>Selecione nossa ISO e marque a opção &ldquo;Colocar CD&rdquo;</p>

<p><img src="http://brunocarvalho.net/images/ovirt/iniciariso.jpg" alt="" /></p>

<p>Agora vamos Abrir o Console da VM para iniciar a Instalação.</p>

<p><img src="http://brunocarvalho.net/images/ovirt/abrirconsole.jpg" alt="" /></p>

<p>Para o Remote Viewer abrir será nescessario baixar O Virt Manager (<a href="https://virt-manager.org/download/">https://virt-manager.org/download/</a>)</p>

<p><img src="http://brunocarvalho.net/images/ovirt/centosinstall.jpg" alt="" /></p>

<p><strong>Troubleshooting:</strong></p>

<p>Resolvendo problema com &ldquo; Failed to acquire lock: No space left on device &rdquo; ao iniciar a vm hosted engine</p>

<pre><code>[root@node1 ~]# vdsClient -s 0 list
.........
.........
exitMessage = Failed to acquire lock: No space left on device
..........
..........
[root@node1 ~]# hosted-engine --vm-poweroff
[root@node1 ~]# sanlock direct init -s hosted-engine:0:/rhev/data-center/mnt/&lt;INTERNAL HE SD&gt;/&lt;SDUUID&gt;/ha_agent/hosted-engine.lockspace:0
[root@node1 ~]# systemctl restart ovirt-ha-agent &amp;&amp; systemctl restart ovirt-ha-broker &amp;&amp; systemctl restart vdsmd &amp;&amp; systemctl restart supervdsmd 
[root@node1 ~]# hosted-engine --vm-start
</code></pre>

<p><strong>Referência:</strong></p>

<p><a href="http://www.ovirt.org/documentation/">http://www.ovirt.org/documentation/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuando transferência com RSYNC no caso de interrupção]]></title>
    <link href="http://brunocarvalho.net/blog/2016/11/17/continuando-transferencia-com-rsync-no-caso-de-interrupcao/"/>
    <updated>2016-11-17T13:38:43-02:00</updated>
    <id>http://brunocarvalho.net/blog/2016/11/17/continuando-transferencia-com-rsync-no-caso-de-interrupcao</id>
    <content type="html"><![CDATA[<p><img src="http://brunocarvalho.net/images/rsync.jpg" alt="rysnc" /></p>

<p>Olá turmadaaa, precisei realizar uma transferência de uma VM do XenServer com 300G que estava em um ponto de montagem NFS para um outro local remoto e para evitar problemas de interrupção, utilizei o comando abaixo que me permiti continuar a transferência em casos de problemas, como perda de conexão etc&hellip;</p>

<pre><code># rsync -vrlPtz --append /mnt/web.xva /storage/export/
    sending incremental file list
    web.xva     4641783808   1%   6.89MB/s    9:57:11
</code></pre>

<p>O Comando acima basicamente irá comprimir, continuar e salvar parcialmente apresentando a saída em detalhes. Pode ser utilizado com apenas um arquivo ou com diretórios.</p>

<p>Caso queira usar com ssh:</p>

<pre><code># rsync -vrlPtz --append -e 'ssh -p 2222' login@host:/tmp/web.xva /tmp/web.xva
</code></pre>

<p>Dica SSH Com Tunnel: ssh -L 2222:hostlocal:2222 root@hostremoto</p>

<hr />

<p><strong>Referência:</strong></p>

<p><a href="https://download.samba.org/pub/rsync/rsync.html">https://download.samba.org/pub/rsync/rsync.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Empregos morrem e empregos nascem]]></title>
    <link href="http://brunocarvalho.net/blog/2016/10/04/empregos-morrem-e-empregos-nascem/"/>
    <updated>2016-10-04T12:10:36-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/10/04/empregos-morrem-e-empregos-nascem</id>
    <content type="html"><![CDATA[<p><img src="http://brunocarvalho.net/images/digitau.jpg" alt="" /></p>

<p>No Brasil funciona assim:</p>

<ul>
<li><p>Itaú Digital = Desemprego</p></li>
<li><p>Uber = Desemprego</p></li>
</ul>


<p>Passamos da era agrícola, industrial, o mundo já está entrando na era pós tecnológica. Porque será que apenas no Brasil não dá certo? Será que nosso País não evoluiu ao ponto de entender que o mundo mudou?</p>

<p>Em vários lugares a era digital foi melhor para o consumidor, está claro que proibir o progresso e o desenvolvimento para manter empregos antiquados será menos efetivo e pior para todos! Alguns dos empregos mais desejados hoje não existiam a 10 anos atrás, estamos entrando em uma era de novas oportunidades, entenda que empregos morrem e empregos nascem.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Alterando Host pelo foreman-rake no Red hat Satellite 6.x]]></title>
    <link href="http://brunocarvalho.net/blog/2016/09/23/alterando-host-pelo-foreman-rake-no-red-hat-satellite-6-dot-x/"/>
    <updated>2016-09-23T15:05:16-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/09/23/alterando-host-pelo-foreman-rake-no-red-hat-satellite-6-dot-x</id>
    <content type="html"><![CDATA[<p>Atualmente tive problema com interface invalida ao atualizar provisionamento do host utulizando o dashboard no satellite 6.2.</p>

<p>Este problema ocorre por alguma mudança na interface que foi atualizado pelo factor do satellite e no momento a interface não existe mais e/ou está inconsistente com os dados fornecidos.</p>

<p>Abaixo executo comandos para contorna esse problema, removendo uma interface invalida no host.</p>

<pre><code># foreman-rake console
Loading production environment (Rails 4.1.5)

irb(main):001:0&gt; host = Host.find_by_name('HOSTNAME')

irb(main):003:0&gt; i = host.interfaces[1]

irb(main):004:0&gt; i.destroy
</code></pre>

<p>Trocando o array &ldquo;host.interfaces[0,1,2]&rdquo; você pode navegar em todas interfaces, veja no dashboard ao editar e atualizar o host, quais estão apresentado problemas e remova pelo foreman-rake.</p>

<p>Caso queira editar algum atributo da interface pelo foreman-rake utilize da seguinte forma.</p>

<pre><code># foreman-rake console
Loading production environment (Rails 4.1.5)

irb(main):001:0&gt; host = Host.find_by_name('HOSTNAME')

irb(main):002:0&gt; i = host.interfaces[0]

irb(main):003:0&gt; i.name = "hostname"

irb(main):004:0&gt; i.save!
</code></pre>

<p>Adpatando os comandos acima pode ser alterar varios atributos do provisionamento pelo foreman-rake.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Iniciando jornada com Satellite 6.2]]></title>
    <link href="http://brunocarvalho.net/blog/2016/08/19/iniciando-jornada-com-satellite-6-dot-2/"/>
    <updated>2016-08-19T14:48:46-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/08/19/iniciando-jornada-com-satellite-6-dot-2</id>
    <content type="html"><![CDATA[<p>Recentemente a Red hat lançou o Satellite 6.2 GA, um produto que ganhou grandes melhorias e novas funcionalidades.</p>

<p><img src="http://brunocarvalho.net/images/satellite/img1.jpg" alt="img1" /></p>

<p>O Satellite se tornou um produto essencial para gerenciamento de ciclo de vida de sistemas da Red Hat em ambientes físicos, virtuais, nuvens privadas e públicas. A ferramenta apresenta implementação remota e estende a capacidade de gestão de pacotes, configurações, máquinas virtuais, containers, segurança entre grandes outras funcionalidades.</p>

<p>Uma das funcionalidades mais interessante dessa nova versão e o Remote Execute e Job Scheduling utilizando ssh. Agora será possível agenda execuções remotas em todos hosts. O OpenSCAP utilizado para auditoria de segurança, agora já vem instalado junto com o Satellite 6.2, antes precisava ser instalado manualmente. Melhorias no console web ficou notório, também percebi que a interface web está mais rápida.</p>

<p>Então amigos administradores de RHEL/CentOS, se você utiliza o Red Hat Satellite 5 e está pensando em implementar o Satellite 6, pode iniciar sua jornada, o  Satellite 6.2 GA está finalmente atingindo o nível de maturidade necessária para gerir ambientes de pequeno e grande porte.</p>

<h1>Migração</h1>

<p>Atualmente estou utilizando a versão 6.1 e abaixo vou listar os passos e dificuldades encontradas no meu ambiente para migração do 6.2 GA.</p>

<blockquote><p>Não será possível realizar migração de 6.2 Beta para 6.2 GA, antes de iniciar a migração para 6.2 verifique se seu ambiente está atualizado e com a versão 6.1.9. Não se esqueça de realizar o backup do ambiente atual.</p></blockquote>

<p>Verificando se existe alguma atualização recente.
Desabilite todos repositórios, deixe apenas os seguintes habilitado:</p>

<pre><code># subscription-manager repos --disable=*
# subscription-manager repos --enable=rhel-7-server-rpms
# subscription-manager repos --enable=rhel-7-server-satellite-6.1-rpms
# yum update
</code></pre>

<blockquote><p>Caso atualize o kernel será necessário reiniciar o sistema</p></blockquote>

<p>Após o update do S.O e dos novos pacotes, o satellite já estará com a versão 6.1.9, execute o seguinte comando para finalizar:</p>

<pre><code># katello-installer --upgrade

Upgrading...
Upgrade Step: stop_services...
Upgrade Step: start_mongo...
Upgrade Step: migrate_pulp...
Upgrade Step: start_httpd...
Upgrade Step: migrate_candlepin...
Upgrade Step: migrate_foreman...
Upgrade Step: Running installer...
Installing Done   [100%] 

[......................................................]
  The full log is at /var/log/katello-installer/katello-installer.log
Upgrade Step: restart_services...
Upgrade Step: db_seed...
Upgrade Step: errata_import (this may take a while) ...
Upgrade Step: update_gpg_urls (this may take a while) ...
Upgrade Step: update_repository_metadata (this may take a while) ...
Katello upgrade completed!
</code></pre>

<p>Vamos realizar um check antes de realizar a migração para o 6.2.</p>

<pre><code># foreman-rake katello:upgrade_check

This script makes no modifications and can be re-run multiple times for the most up to date results.
Checking upgradeability...

Checking for running tasks...
[FAIL] - There are 35 active tasks.

Checking the current version...
[PASS] - Current version of the Katello Ruby RPM is 2.2.0.92 and needs to greater than or equal to 2.2.0.90

Checking content hosts...
Calculating Host changes on upgrade.  This may take a few minutes.


Summary:
Content hosts to be preserved: 169
Content hosts to be deleted: 3
Details on Content Hosts planned for deletion saved to /tmp/pre-upgrade-1471033031.csv
You may want to manually resolve some of the conflicts and duplicate Content Hosts.
Upon upgrade those found for deletion will be removed permanently.
</code></pre>

<p>Como podemos ver minha migração apresentará falha <strong>&ldquo; [FAIL] - There are 35 active tasks.&rdquo;</strong> Resolvendo active tasks provável lock, podemos ir manualmente na Web gui Monitor->Tasks e filtrar pelas tasks que estão em lock, assim executando manualmente os skip, também podemos identificar pelo hammer.</p>

<p>Apresenta um resumo das tasks em pausa</p>

<pre><code># hammer task resume --search "state=paused"
</code></pre>

<p>Apresenta todas tasks em pausa</p>

<pre><code># hammer task list --search "state=paused"
</code></pre>

<p>Removendo tasks em lock pelo foreman-rake</p>

<pre><code># service foreman-tasks stop  
# foreman-rake console

irb(main):001:0&gt; ForemanTasks::Task.where(:state =&gt; :planned).where(:label =&gt; "Actions::Katello::Repository::Sync").destroy_all
irb(main):002:0&gt; ForemanTasks::Task.where(:state =&gt; :planned).where(:label =&gt; "Actions::Katello::System::GenerateApplicability").destroy_all
irb(main):003:0&gt; ForemanTasks::Task.where(:state =&gt; :paused).where(:label =&gt; "Actions::Katello::Repository::Sync").destroy_all
irb(main):004:0&gt; ForemanTasks::Task.where(:state =&gt; :paused).where(:label =&gt; "Actions::Katello::System::GenerateApplicability").destroy_all
irb(main):005:0&gt; ForemanTasks::Task.where(:label =&gt; "Actions::Candlepin::ListenOnCandlepinEvents").destroy_all

Caso nescessário remova a task pelo id
irb(main):001:0&gt; ForemanTasks::Task.find("799bc5fb-2d4c-4d0d-9d7d-4e42e9a8ace8").destroy

Saindo do Foreman rake
irb(main):002:0&gt; exit
# service foreman-tasks start 
</code></pre>

<p>Reindexando o banco de dados para evitar futuras inconsistências</p>

<pre><code># foreman-rake katello:reindex
# katello-service restart
</code></pre>

<p>Executando novamente o upgrade_check podemos notar que as  pendência de tasks foram solucionadas.</p>

<pre><code># foreman-rake katello:upgrade_check

This script makes no modifications and can be re-run multiple times for the most up to date results.
Checking upgradeability...

Checking for running tasks...
[PASS] - There are 0 active tasks.

Checking the current version...
[PASS] - Current version of the Katello Ruby RPM is 2.2.0.92 and needs to greater than or equal to 2.2.0.90

Checking content hosts...
Calculating Host changes on upgrade.  This may take a few minutes.


Summary:
Content hosts to be preserved: 169
Content hosts to be deleted: 3
Details on Content Hosts planned for deletion saved to /tmp/pre-upgrade-1471269446.csv
You may want to manually resolve some of the conflicts and duplicate Content Hosts.
Upon upgrade those found for deletion will be removed permanently.
</code></pre>

<h1>Iniciando processo de upgrade 6.2</h1>

<p>Desabilitar repositório Satellite 6.1</p>

<pre><code># subscription-manager repos --disable rhel-7-server-satellite-6.1-rpms
</code></pre>

<p>Habilitando repositórios necessário</p>

<pre><code># subscription-manager repos --enable rhel-7-server-satellite-6.2-rpms

# subscription-manager repos --enable=rhel-server-rhscl-7-rpms
</code></pre>

<p>Parando serviço</p>

<pre><code># katello-service stop
</code></pre>

<p>Limpando cache yum</p>

<pre><code># yum clean all
</code></pre>

<p>Upgrade de todos pacotes do sistema</p>

<pre><code># yum update
</code></pre>

<p>Upgrade do ambiente satellite</p>

<pre><code># satellite-installer --scenario satellite --upgrade
</code></pre>

<p>Nessa etapa acima encontrei um erro de migração que estava relacionado a ambiente puppet orfão, caso ocorra um erro parecido, tente remove o ambiente puppet informado na msg e execute o comando novamente.</p>

<blockquote><p>foreman-rake katello:upgrades:3.0:update_puppet_repository_distributors
Updating Puppet Repository Distributors
Updating Content View Puppet Environment Distributors
rake aborted!
ForemanTasks::TaskError: Task 541787f6-ad56-449a-be6f-1b2001fc3538: Katello::Errors::PulpError: PLP0034: The distributor Library-ccv_rhel72_puppet-jboss indicated a failed response when publishing repository <strong>Library-ccv_rhel72_puppet-jboss</strong>.</p></blockquote>

<p>Iniciando serviço</p>

<pre><code># katello-service start
</code></pre>

<p>Verificando serviço</p>

<pre><code># hammer ping
candlepin:
Estado:          ok
Server Response: Duration: 19ms
candlepin_auth:
Estado:          ok
Server Response: Duration: 23ms
pulp:
Estado:          ok
Server Response: Duration: 66ms
foreman_tasks:
Estado:          ok
Server Response: Duration: 882ms
</code></pre>

<p>Verique também na interface web se todos os serviços estão ok, na aba Administer->About</p>

<p>Caso ao logar na interface Web, apresente um erro de &ldquo;undefined method `label' for nil:NilClass&rdquo; atualize o pacote tfm-rubygem-katello e reinicie o serviço. (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1361793">https://bugzilla.redhat.com/show_bug.cgi?id=1361793</a>)</p>

<p><img src="http://brunocarvalho.net/images/satellite/img2.jpg" alt="img2" /></p>

<pre><code># rpm -Uvh tfm-rubygem-katello-3.0.0.73-1.el7sat.noarch.rpm
# katello-service restart
</code></pre>

<p><img src="http://brunocarvalho.net/images/satellite/img3.jpg" alt="img3" /></p>

<h1>Upgrade Client Satellite 6.2</h1>

<p>Desabilite repositório antigo</p>

<pre><code># subscription-manager repos --disable rhel-7-server-satellite-tools-6.1-rpms
</code></pre>

<p>Habilite novo repositório</p>

<pre><code># subscription-manager repos --enable=rhel-7-server-satellite-tools-6.2-rpms
</code></pre>

<p>Realize upgrade do katello-agent</p>

<pre><code># yum upgrade katello-agent
</code></pre>

<h1>Referencias:</h1>

<ul>
<li><p><a href="https://access.redhat.com/blogs/1169563/posts/2464761">https://access.redhat.com/blogs/1169563/posts/2464761</a></p></li>
<li><p><a href="https://access.redhat.com/documentation/en/red-hat-satellite/6.2/single/installation-guide">https://access.redhat.com/documentation/en/red-hat-satellite/6.2/single/installation-guide</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redimensionar disco no VMware com LVM utilizando RHEL/CentOS - On the Fly]]></title>
    <link href="http://brunocarvalho.net/blog/2016/06/22/redimensionar-disco-no-vmware-com-lvm-utilizando-rhel-slash-centos-on-the-fly/"/>
    <updated>2016-06-22T09:30:26-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/06/22/redimensionar-disco-no-vmware-com-lvm-utilizando-rhel-slash-centos-on-the-fly</id>
    <content type="html"><![CDATA[<p>Uma das demandas que executo diariamente está relacionado a expansão de disco em máquinas virtuais. Muitas vezes a máquina não pode ser reiniciada e todo procedimento precisa ser executado em produção.</p>

<blockquote><p>Sempre execute um snapshot da VM antes de executar os procedimentos abaixo.</p></blockquote>

<p>Primeiro passo temos que aumentando nosso disco, no Vmware.</p>

<p>Selecione a VM no seu vSphere depois, vá em “Edit Settings”, selecione o “Virtual Disk” que deseje aumentar. No meu caso aumentarei 20G.</p>

<p>Verificando as partições do sistema:</p>

<pre><code># df -h

FilesystemSize  Used Avail Use% Mounted on
/dev/mapper/vg_web01-lv_root 45G  1.5G   42G   4% /
tmpfs 1.9G 0  1.9G   0% /dev/shm
/dev/sda1 477M   78M  374M  18% /boot
</code></pre>

<p>Partição que sera expandida (<strong>/dev/mapper/vg_web01-lv_root</strong>)</p>

<p>Todo procedimento será realizado com a VM ligada e a partição montada.</p>

<p>Vamos agora realizar um procedimento para que o seu Linux reconheça o novo espaço adicionado sem precisar do reboot.</p>

<pre><code># ls /sys/class/scsi_device/
0:0:0:0    2:0:0:0 
# echo 1 &gt; /sys/class/scsi_device/0\:0\:0\:0/device/rescan
</code></pre>

<p>No meu caso tenho duas controladoras e meu disco se encontra na primeira. Pronto agora quando for executar o cfdisk ou fdisk você já consegue visualizar o espaço adicionado no Vmware, no meu caso foi criado o /dev/sda3.</p>

<blockquote><p>Caso seja um novo Virutal Disk, execute os comando abaixo para identificar o novo device no seu ambiente sem precisar realizar o reboot.</p>

<pre><code>Buscando host bus number
# grep mpt /sys/class/scsi_host/host?/proc_name
/sys/class/scsi_host/host0/proc_name:mptspi

Execute o comando abaixo no host encontrado
# echo "- - -" &gt; /sys/class/scsi_host/host0/scan
</code></pre></blockquote>

<p>Vamos utilizar o cfdisk para criar uma nova partição com o espaço disponível do tipo LVM(8e)</p>

<pre><code># cfdisk /dev/sda3 (a utilização do cfdisk não será abordada passo-a-passo)
</code></pre>

<p><img src="http://brunocarvalho.net/images/cfdisk.JPG" alt="cfdisk" /></p>

<p>Após a criação da nova partição, execute o comando abaixo</p>

<pre><code># partprobe /dev/sda (RHEL7)
# partx -a /dev/sda (RHEL6)
</code></pre>

<p>Podemos visualizar com o comando abaixo a nova partição reconhecida pelo sistema chamada <strong>sda3</strong></p>

<pre><code> # cat /proc/partitions

major minor  #blocks  name

   80   73400320 sda
   81     512000 sda1
   82   51915776 sda2
   83   20971520 sda3
 2530   47849472 dm-0
 25314063232 dm-1
</code></pre>

<p>Após a nova partição ser reconhecida vamos adiciona ao LVM.</p>

<pre><code># pvcreate /dev/sda3    

Physical volume "/dev/sda3" successfully created 
</code></pre>

<p>Expandindo grupo vg_web01</p>

<pre><code># vgextend vg_web01 /dev/sda3

Volume group "vg_web01" successfully extended
</code></pre>

<p>Expandindo Partição vg_web01-lv_root</p>

<pre><code># lvextend -L+20GB /dev/mapper/vg_web01-lv_root

  Size of logical volume vg_web01/lv_root changed from 45.63 GiB (11682 extents) to 65.63 GiB (16546 extents).
  Logical volume lv_root successfully resized.
</code></pre>

<p>Redimensionar sistema de arquivo ext4</p>

<pre><code># resize2fs /dev/vg_web01/lv_root

resize2fs 1.41.12 (17-May-2010)
Filesystem at /dev/vg_web01/lv_root is mounted on /; on-line resizing required
old desc_blocks = 3, new_desc_blocks = 5
Performing an on-line resize of /dev/vg_web01/lv_root to 16943104 (4k) blocks.
The filesystem on /dev/vg_web01/lv_root is now 16943104 blocks long.
</code></pre>

<p>Podemos agora verificar que a partição foi redimensionada para 65G sem precisar reiniciar ou muito menos desmontar.</p>

<pre><code># df -h
FilesystemSize  Used Avail Use% Mounted on
/dev/mapper/vg_web01-lv_root 65G  1.5G   59G   3% /
tmpfs 1.9G 0  1.9G   0% /dev/shm
/dev/sda1 477M   78M  374M  18% /boot
</code></pre>

<p>Caso sua partição seja xfs basta executar o seguinte comando</p>

<pre><code># xfs_growfs /dev/vg_web01/lv_root
</code></pre>

<h5>Referências:</h5>

<ul>
<li><p><a href="http://tldp.org/HOWTO/LVM-HOWTO/index.html">http://tldp.org/HOWTO/LVM-HOWTO/index.html</a></p></li>
<li><p><a href="https://blogs.it.ox.ac.uk/oxcloud/2013/03/25/rescanning-your-scsi-bus-to-see-new-storage/">https://blogs.it.ox.ac.uk/oxcloud/2013/03/25/rescanning-your-scsi-bus-to-see-new-storage/</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitorando Weblogic em DomainRuntime com Zabbix]]></title>
    <link href="http://brunocarvalho.net/blog/2016/06/16/monitorando-weblogic-em-domainruntime-com-zabbix/"/>
    <updated>2016-06-16T13:51:28-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/06/16/monitorando-weblogic-em-domainruntime-com-zabbix</id>
    <content type="html"><![CDATA[<p>Sempre monitorei Jboss com zabbix, mas recentemente recebi uma demanda e encontrei algumas dificuldades que gostaria de compartilhar com a comunidade.
Esse cenário foi feito no RHEL6, Weblogic 11g com jrockit 1.6, zabbix 2.4, porem entendendo o cenário, pode ser customizado para outras versões.</p>

<p>A Oracle tem um servidor Mbean chamado DomainRuntime, que está disponível no AdminServer. Conectando-se nesse servidor é possível coletar todas informações das JVM e do domínio. Assim não será necessário exportar JMX de cada JVM.
 Com essa solução ganha-se tempo de configuração, segurança, melhor administração de itens e gráficos agregados, além de não haver necessidade de abrir porta JMX em nenhuma JVM.
Então, se tenho um domínio com 10 instancias(JVM), será possível apenas com a URL do console admin pegar todos Mbeans desse domínio.</p>

<h3>Servidores MBean em Weblogic</h3>

<p>O Middleware Weblogic  é composto por três MBeanServers próprios que são exportados via RMI/IIOP como JSR-160. Estes podem ser consultados por meio de nome JNDI como mostra a lista abaixo. Além disso, existe a PlatformMBeanServer que pode ser exportado juntamente com o MbeanServer do weblogic.</p>

<ul>
<li><strong>Domain Runtime MBean Server</strong></li>
<li><strong>Runtime MBean Server</strong></li>
<li><strong>Edit MBean Server</strong></li>
</ul>


<p>O MbeanServer que vamos utilizar para buscar toda árvore do domínio weblogic será o Domínio Runtime MBean Servidor (weblogic.management.mbeanservers.domainruntime). Esse Mbean só está disponível na JVM do AdminServer.</p>

<h5>Ative os seguintes itens abaixo no AdminServer do Weblogic:</h5>

<pre><code>Domínio-&gt;Geral-&gt;Avançado 

- Servidor MBean de Compatibilidade Ativado
- Servidor MBean da Plataforma Ativado
- Servidor MBean da Plataforma Usado
</code></pre>

<p><img src="http://brunocarvalho.net/images/zabbix/img1.png" alt="img1" /></p>

<p>Entre em cada JVM e adicione a seguinte linha no argumento que se encontra na aba Inicialização dos servidores</p>

<pre><code>Domínio-&gt;Ambientes-&gt;Servidores-&gt;”NAME JVM”-&gt;Inicialização do Servidor

-Djavax.management.builder.initial=weblogic.management.jmx.mbeanserver.WLSMBeanServerBuilder
</code></pre>

<p><img src="http://brunocarvalho.net/images/zabbix/img2.png" alt="img2" /></p>

<blockquote><p><strong><em>Será necessário reiniciar o AdminServer e as JVM do domínio.</em></strong></p></blockquote>

<h3>Exportando RMI/IIOP AdminServer</h3>

<p>Para facilitar a configuração, vamos utilizar a leitura dos Mbeans como anonymous, mas também poderíamos utilizar autenticação fixada no  JNDI.</p>

<p>Permitir anonymous acesso de  leitura, caso deseja monitorar sem autenticação no AdminServer.</p>

<pre><code>Domínio-&gt;Segurança-&gt;Geral  - Marque o "Acesso Anônimo Ativado”
</code></pre>

<p><img src="http://brunocarvalho.net/images/zabbix/img3.png" alt="img3" /></p>

<h3>Habilitar o IIOP no manager AdminServer</h3>

<pre><code>Dominio-&gt;Ambientes-&gt;Servidores-&gt;AdminServer-&gt;Protocolos-&gt;IIOP 
</code></pre>

<p><img src="http://brunocarvalho.net/images/zabbix/img4.png" alt="img4" /></p>

<blockquote><p><strong><em>Será necessário reiniciar o AdminServer.</em></strong></p></blockquote>

<p>Agora abra o jconsole com os seguintes parâmetros:</p>

<pre><code>jconsole -J- Djava.class.path=$JAVA_HOME/lib/jconsole.jar:$JAVA_HOME/lib/tools.jar$WL_HOME/server/lib/wljmxclient.jar -J-Djmx.remote.protocol.provider.pkgs=weblogic.management.remote
</code></pre>

<p>Use a URL de serviço JMX via IIOP DomainRuntime:</p>

<pre><code>service:jmx:rmi:///jndi/iiop://IPADMINSERVER:7001/weblogic.management.mbeanservers.domainruntime
</code></pre>

<p>Primeiro tente se conectar utilizando o login e a senha do AdminServer e veja se consegue ler a arvore com.bea/DomainRuntimeService. Depois tente sem autenticação e veja se consegue ler via anonymous.</p>

<p><img src="http://brunocarvalho.net/images/zabbix/img5.png" alt="img5" /></p>

<p>Caso não consiga ler como anonymous vamos alterar a permissão do JNDI.</p>

<pre><code>1. Entre no AdminConsole(http://IP:7001/), click no AdminServer -&gt; Exibir Árvore JNDI
2. Vá para o weblogic-&gt;management 
3. Click no mbeanservers 
4. Click em Segurança-&gt;Politicas 
5. Escolha o Methods= lookup e adicione a politica "Allow access to everyone" 
6. Restart AdminServer
7. Abra o jconsole com os parâmetros informados 
8. Conecte novamente URL : service:jmx:rmi:///jndi/iiop://IPADMINCSERVER:7001/weblogic.management.mbeanservers.domainruntime
</code></pre>

<p><img src="http://brunocarvalho.net/images/zabbix/img6.png" alt="img6" /></p>

<h3>Modificação do external script jmx_discovery para DomainRuntime</h3>

<p>Após Conseguir ler a arvore DomainRuntime do AdminServer com jconsole, vamos alterar o external script para realizar as coletas.</p>

<p>External Script original: <a href="https://github.com/RiotGamesMinions/zabbix_jmxdiscovery" target="_blank">github.com/RiotGamesMinions/zabbix_jmxdiscovery</a></p>

<p>Modificações que foram feita na class JMXDiscovery.java libs adicionadas:</p>

<pre><code>import java.io.PrintStream;
import javax.naming.*;
</code></pre>

<p>Alteração na linha 46:</p>

<pre><code>this.jmxServerUrl = new JMXServiceURL("service:jmx:rmi:///jndi/rmi://" + hostname + ":" + port + "/jmxrmi");
</code></pre>

<p>Para:</p>

<pre><code>this.jmxServerUrl = new JMXServiceURL("service:jmx:rmi:///jndi/iiop://" + hostname + ":" + port + "/weblogic.management.mbeanservers.domainruntime");
</code></pre>

<p>Como o DomainRuntime se conecta com IIOP e utiliza algumas libs especificar, foi necessário adicionar o pacote wlfullclient.jar(Pacote encontrado no servidor weblogic)</p>

<p>Coloque o  wlfullclient.jar na pasta lib do pacote zabbix_jmxdiscovery.  Após esses ajustes recompile o pacote utilizando ant.</p>

<blockquote><p><strong><em>Não irei aborta a utilização do <a href="http://ant.apache.org" title="ant">ant</a>, pois não e proposito deste post. Futuramente posso está criando um post especifico.</em></strong></p></blockquote>

<p><strong><em>Obs: O /etc/hosts precisa estar resolvendo o nome da própria máquina local</em></strong></p>

<p>Vá para diretório do binário compilado que foi realizado as modificações do jmx_discovery e execute o comando abaixo:</p>

<pre><code>[brunocarvalho@zabbix zabbix_jmxdiscovery]# ./jmx_discovery com.bea:Name=DomainRuntimeService,Type=* 192.168.10.1:7001
{"data":[{"{#PROPTYPE}":"weblogic.management.mbeanservers.domainruntime.DomainRuntimeServiceMBean","{#JMXOBJ}":"com.bea:Name=DomainRuntimeService,Type=weblogic.management.mbeanservers.domainruntime.DomainRuntimeServiceMBean","{#JMXDESC}":"&lt;p&gt;Provides a common access point for navigating to all runtime and configuration MBeans in the…
</code></pre>

<p>Se a saída for parecida com a de cima seu external script está funcional.</p>

<h3>Modificação do Zabbix Java Gateway para DomainRuntime</h3>

<p>Para que o zabbix-java-gateway comece a coletar utilizando o DomainRuntime, será  necessário recompilar o jar do zabbix, alterando a url do jmx na class JMXItemChecker.java.</p>

<p>Vamos precisar colocar a lib wlfullclient.jar na pasta src para compilar o zabbix-java-gateway</p>

<blockquote><p><strong><em>Não irei aborta a compilação do <a href="https://www.zabbix.com/documentation/2.4/manual/installation/install" target="_blank">Zabbix</a>, pois não é proposito deste post. Futuramente posso está criando um post especifico.</em></strong></p></blockquote>

<p>Fiz alterações simples para atender minha demanda, mas pode ser melhorada, de uma olhada no seguinte link: <a href="https://support.zabbix.com/browse/ZBXNEXT-1274" target="_blank">support.zabbix.com/browse/ZBXNEXT-1274</a></p>

<p><strong>Class alterada:</strong>
<em>/opt/install/zabbix-2.4.1/src/zabbix_java/src/com/zabbix/gateway/JMXItemChecker.java</em></p>

<pre><code>public JMXItemChecker(JSONObject request) throws ZabbixException
    {
         super(request);
            try
            {
                    String conn = request.getString(JSON_TAG_CONN);
                    int port = request.getInt(JSON_TAG_PORT);

                    Integer remoting = new Integer("7777");
                    Integer weblogic = new Integer("7001");

                    int retvaljboss = remoting.compareTo(port);
                    int retvalweblogic = weblogic.compareTo(port);
                if (retvaljboss == 0)
            {
   //suporta jboss7 na porta jmx 7777        
                url = new JMXServiceURL("service:jmx:remoting-jmx://" + conn + ":" + port);
            }
                if (retvalweblogic == 0)
            {
                 url = new JMXServiceURL("service:jmx:rmi:///jndi/iiop://" + conn + ":" + port + "/weblogic.management.mbeanservers.domainruntime");
            }
              else
            {url = new JMXServiceURL("service:jmx:rmi:///jndi/rmi://" + conn + ":" + port + "/jmxrmi");
            }
</code></pre>

<p>Agora sua imaginação não tem limites! Basta configurar seu zabbix para fazer LLD no server Domainruntime do weblogic utilizando o jmx_discovery igualmente como é feito no jmxrmi.</p>

<pre><code>1. Adicione o host na interface jmx com o ip do AdminConole na porta 7001
2. Adicione o template weblogic anexo no host
3. Adicione macro para o host

{$ADMINSERVER} - ipadminserver:7001 
{$DOMINIO}  - nomedoseudominio
</code></pre>

<p>Segue anexo arquivos utilizados:</p>

<p><a href="https://github.com/brunowcs/zabbix_weblogic/" target="_blank">github.com/brunowcs/zabbix_weblogic/</a></p>

<p>O .RAR ficou um pouco grande por conta dos binários java, então tive que dividir em 3 partes para o github aceitar o upload.</p>

<ul>
<li><p>Template Weblogic.xml LLD com 42 itens, 4 triggers, 16 gráficos criado para weblogic DomainRuntime (Não esqueça de configurar as macros)</p></li>
<li><p>JMXDiscovery.jar com alteração da class JMXDiscovery.java do zabbix_jmxdiscovery, recopilação alterações para connect IIOP com inclusão da lib própria do weblogic para comunicação do server Domainruntime</p></li>
<li><p>Bash do jmx_discovery para se colocar junto com o JMX na pasta do externalscripts do zabbix</p></li>
<li><p>zabbix-java-gateway-2.4.1.jar alteração da class  JMXItemChecker.java do zabbix-java-gateway, compilação alterações para connect IIOP com inclusão da lib própria do weblogic comunicação do server  Domainruntime</p></li>
<li><p>wlfullclient.jar (lib utilizada na compilação)</p></li>
<li><p>org-json-2010-12-28.jar (lib utilizada na compilação)</p></li>
</ul>


<blockquote><p>Recomendo realizar testes no seu em ambiente de homologação antes de entrar em produção</p></blockquote>

<p>Resultado:</p>

<p><img src="http://brunocarvalho.net/images/zabbix/resultadofinal.png" alt="resultadofinal" /></p>

<h3>Referências</h3>

<ul>
<li><a href="https://docs.oracle.com/cd/E21764_01/web.1111/e13728/accesswls.htm#JMXCU144">https://docs.oracle.com/cd/E21764_01/web.1111/e13728/accesswls.htm#JMXCU144</a></li>
<li><a href="https://blogs.oracle.com/theshortenspot/entry/accessing_jmx_for_oracle_weblo">https://blogs.oracle.com/theshortenspot/entry/accessing_jmx_for_oracle_weblo</a></li>
<li><a href="https://github.com/RiotGamesMinions/zabbix_jmxdiscovery">https://github.com/RiotGamesMinions/zabbix_jmxdiscovery</a></li>
<li><a href="https://www.zabbix.com/documentation/2.4/manual/installation/install">https://www.zabbix.com/documentation/2.4/manual/installation/install</a></li>
<li><a href="https://support.zabbix.com/browse/ZBXNEXT-1274">https://support.zabbix.com/browse/ZBXNEXT-1274</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prazer Bruno Carvalho!]]></title>
    <link href="http://brunocarvalho.net/blog/2016/06/13/prazer-bruno-carvalho/"/>
    <updated>2016-06-13T15:34:46-03:00</updated>
    <id>http://brunocarvalho.net/blog/2016/06/13/prazer-bruno-carvalho</id>
    <content type="html"><![CDATA[<p>Olá pessoal,</p>

<p>Nesse meu primeiro post, vou falar um pouco sobre o  intuito desse blog.</p>

<p>Bem&hellip; para começar, as escritas por aqui serão bem informal e técnicas, então prenda-se apenas o proposito.</p>

<p>Essa ideia já é velha, mas venho adiando por muitos longos anos, porem recentemente tive a necessidade de relembrar alguns procedimentos executados há um bom tempo atrás, e fiquei tanto tempo pesquisando nas minhas mídias externas e na internet que agora tomei a iniciativa de documentar tudo em um só lugar, assim quando quiser compartilhar algo e relembrar algo pesquiso aqui!  :)</p>

<p>Concluindo&hellip; esse será meu disco virtual, onde vou compartilhar com todos meus estudos, ideias, implementações e procedimentos do meu dia a dia&hellip;</p>

<p>Qualquer coisa comente, critique, ajude a enrique os posts o importante é a troca de experiências e conhecimentos.</p>

<p>Quem sou eu? <a href="http://brunocarvalho.net/about/index.html">Sobre</a>.</p>
]]></content>
  </entry>
  
</feed>
